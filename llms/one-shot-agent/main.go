package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/tmc/langchaingo/agents"
	"github.com/tmc/langchaingo/chains"
	"github.com/tmc/langchaingo/llms/googleai"
	"github.com/tmc/langchaingo/tools"
	"github.com/tmc/langchaingo/tools/wikipedia"
)

const modelName = "gemini-2.0-flash-exp"
const apiKey = "AIzaSyB-PzChELpgq2XfXzjg5XWYkhbmtlfkVkM"

var llmClient, _ = googleai.New(context.Background(), googleai.WithAPIKey(apiKey), googleai.WithDefaultModel(modelName))

type translator struct{}

// It is important to have a very clear name and a single word
// This name should be unique across all tools used by the agent
func (t translator) Name() string {
	return "translator"
}

// Use very clear description. What the tool should do and its input which is important for formatting
func (t translator) Description() string {
	return `Useful in translating text.
	The input to this tool should be of the following format:
	{
		"lang": "<ISO_LANGUAGE>",
		"text: "<TEXT_TO_TRANSLATE>"
	}

	Input json should be built from API reference and the following instructions:

	- lang (required): The language the text should be translated to. Format should be ISO 639 set 1
	- text (required): The text to be translated
	`
}

func (t translator) Call(ctx context.Context, input string) (string, error) {
	var parsedInput struct {
		Lang string `json:"lang"`
		Text string `json:"text"`
	}

	if err := json.Unmarshal([]byte(input), &parsedInput); err != nil {
		return "", err
	}

	// translate the parsed input and return the result
	text := fmt.Sprintf("Translate %s to %s", parsedInput.Text, parsedInput.Lang)
	return llmClient.Call(context.Background(), text)
}

func run() error {
	ctx := context.Background()

	wikipedia := wikipedia.New("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36")

	agentTools := []tools.Tool{
		tools.Calculator{},
		wikipedia,
		translator{},
	}

	var toolsDescription string

	for _, tool := range agentTools {
		toolsDescription += fmt.Sprintf(`%s: %s\n`, tool.Name(), tool.Description())
	}

	// We can have a template instruction that contains the input tools and question
	// It should be clear. It should have a prefix, format instruction and a suffix
	// When constructing a custom system promp, it is important to make sure the format instructions tells the LLM
	// to return fields "Action" and "Observation" as shown here https://github.com/tmc/langchaingo/blob/238d1c713de3ca983e8f6066af6b9080c9b0e088/agents/mrkl_prompt.go
	// We cannot change the output key for "intermediateSteps" when ReturnIntermediateSteps == true hence we need to make sure that we conform
	// to the output format to avoid parsing issues
	// prompt := prompts.NewPromptTemplate(TEMPLATE_INSTRUCTION, []string{"tools", "question"})
	// if err != nil {
	// 	log.Fatal(err)
	// }

	// This is a simple agent. It is meant for running a task and hence does not have memory nor context
	// Also cannot engage users in long conversations while remembering the context of the conversation
	// The documentation for the NewOneShotAgent methods shows that we can pass agents.WithMaxIterations() but this is indeed false
	// because it does nothing with it. Check what "options" field are actually used in a struct to get an idea of the actual ...Option params

	// Also with agents, we get the actual results without any other texts that might be generated by the LLM
	// <========== THIS AGENT IS OPTIMIZED TO USE TOOLS TO FIGURE OUT STUFF ===========>
	// We can also pass a prompt i.e system prompt to this method and it will override the default prompt used by the agent which is created
	// in this file https://github.com/tmc/langchaingo/blob/238d1c713de3ca983e8f6066af6b9080c9b0e088/agents/mrkl_prompt.go
	// In our prompt, we will have control over the inputVariables
	agent := agents.NewOneShotAgent(llmClient, agentTools, agents.WithOutputKey("result"))
	executor := agents.NewExecutor(agent, agents.WithMaxIterations(6), agents.WithReturnIntermediateSteps())

	// You can call the agent directly by passing the input
	// result1, _ := executor.Call(ctx, map[string]any{"input": "What is 6 + 3 "})

	// This is a very simple chain, more like a basic chain. In this case
	// our chain runs our agent executor which is also a chain since it implements methods in the Chain interface
	// chanins  also exposes Apply, Call etc commands
	// chains.Run(ctx, executor, promptMessage)

	// The Call method can be used when we want to get the output as a map
	// Streaming can only be achieved when we use chains.NewLLMChain or the NewLLMMathChain, basically any chain that uses llm.Generate... impplicitly
	output, err := chains.Call(ctx, executor, map[string]any{"input": "Can you translate hello to german?"})

	// If we want to do something with the intermediate steps,
	// We can use type assertions i.e steps, ok := output["intermediateSteps"].(schema.AgentStep)
	fmt.Printf("The output is %v.\n\nThe thought process of the agent was %+v.\n\nThe type of intermediate steps is %T", output["result"], output["intermediateSteps"], output["intermediateSteps"])

	return err
}

func main() {
	if err := run(); err != nil {
		fmt.Fprintln(os.Stderr, err)
		os.Exit(1)
	}
}
